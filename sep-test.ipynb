{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import nnunet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(dp, gt, keep_dims=True):\n",
    "    vpos_sets, vneg_sets = [],[]\n",
    "    for i in range(dp.shape[0]):\n",
    "        pos_voxels = np.argwhere(gt[i, 0] > 0)\n",
    "        neg_voxels = np.argwhere(gt[i, 0] == 0)\n",
    "        \n",
    "        pos_set = dp[i, :, pos_voxels[:, 0], pos_voxels[:, 1], pos_voxels[:, 2]]\n",
    "        neg_set = dp[i, :, neg_voxels[:, 0], neg_voxels[:, 1], neg_voxels[:, 2]]\n",
    "\n",
    "        vpos_sets.append(pos_set)\n",
    "        vneg_sets.append(neg_set)\n",
    "\n",
    "    vpos_sets = np.vstack(vpos_sets)    \n",
    "    vneg_sets = np.vstack(vneg_sets)\n",
    "\n",
    "    if keep_dims: \n",
    "        return vpos_sets, vneg_sets\n",
    "    \n",
    "    return vpos_sets.mean(axis=1), vneg_sets.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_sep(pos, neg, n_components=2):\n",
    "    if isinstance(n_components, str):\n",
    "        # infer the number of components\n",
    "        pass \n",
    "\n",
    "    if len(pos.shape) < 2:\n",
    "        pos = np.expand_dims(pos, 1)\n",
    "        neg = np.expand_dims(neg, 1)\n",
    "\n",
    "    clf = mixture.GaussianMixture(n_components=n_components, covariance_type=\"full\")\n",
    "    clf.fit(np.concatenate((pos, neg)))\n",
    "\n",
    "    return clf, clf.means_, clf.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sep(clf, preds):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "def visualize_scalar_dist(s, nbins=30, c='b', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    p, x = np.histogram(s, bins=nbins) # bin it into n = N//10 bins\n",
    "    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers\n",
    "    f = UnivariateSpline(x, p, s=nbins)\n",
    "    ax.plot(x, f(x), c=c)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def visualize_multivar_dist():\n",
    "    pass\n",
    "\n",
    "def visualize_dist(s, *args, **kwargs):\n",
    "    if len(s.shape) > 1 or (len(s.shape) == 2 and s.shape[1] != 1):\n",
    "        visualize_scalar_dist(s, *args, **kwargs)\n",
    "    else:\n",
    "        visualize_multivar_dist(s, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, w, h, d):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes \n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.depth = d\n",
    "        self.classifier = torch.nn.Conv3d(in_channels, num_classes, (1,1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.depth, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,4,1,2, 3)\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, epochs, optimizer, criterion):\n",
    "  # put model in training mode\n",
    "  model.train()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        # labels = batch[\"labels\"].to(device)\n",
    "        dp, gt = batch\n",
    "\n",
    "        # forward pass\n",
    "        out_logits = model(dp)\n",
    "        if out_logits.size() != gt.size():\n",
    "            while out_logits.size() != gt.size():\n",
    "                out_logits = F.interpolate(out_logits, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "            \n",
    "        dice_loss = criterion(out_logits, gt)\n",
    "        dice_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from Upstream.nnunet.training.loss_functions.dice_loss import SoftDiceLoss\n",
    "\n",
    "model = LinearClassifier()\n",
    "\n",
    "train_dataloader = None\n",
    "\n",
    "\n",
    "# training hyperparameters\n",
    "# NOTE: I've just put some random ones here, not optimized at all\n",
    "# feel free to experiment, see also DINOv2 paper\n",
    "learning_rate = 5e-5\n",
    "epochs = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = SoftDiceLoss()\n",
    "\n",
    "# put model on GPU (set runtime to GPU in Google Colab)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "train(model, train_dataloader, epochs=epochs, optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
