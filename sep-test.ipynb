{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def extract(dp, gt, keep_dims=True):\n",
    "\n",
    "    while dp.size() != gt.size():\n",
    "        dp = F.interpolate(dp, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "\n",
    "    print(dp.size())\n",
    "    vpos_sets, vneg_sets = [],[]\n",
    "    for i in range(dp.size(0)):\n",
    "        pos_voxels = torch.argwhere(gt[i, 0] > 0)\n",
    "        neg_voxels = torch.argwhere(gt[i, 0] == 0)\n",
    "        \n",
    "        pos_set = dp[i, :, pos_voxels[:, 0], pos_voxels[:, 1], pos_voxels[:, 2]]\n",
    "        neg_set = dp[i, :, neg_voxels[:, 0], neg_voxels[:, 1], neg_voxels[:, 2]]\n",
    "\n",
    "        vpos_sets.append(pos_set)\n",
    "        vneg_sets.append(neg_set)\n",
    "\n",
    "    print([v.size() for v in vpos_sets])\n",
    "    vpos_sets = torch.concat(vpos_sets, -1)    \n",
    "    vneg_sets = torch.concat(vneg_sets, -1)\n",
    "\n",
    "    if keep_dims: \n",
    "        return vpos_sets, vneg_sets\n",
    "    \n",
    "    return vpos_sets.mean(dim=0), vneg_sets.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm(pos, neg, n_components=2):\n",
    "    if isinstance(n_components, str):\n",
    "        # infer the number of components\n",
    "        pass \n",
    "\n",
    "    if not isinstance(pos, np.ndarray):\n",
    "        pos = pos.detach().cpu().numpy()\n",
    "\n",
    "    if not isinstance(neg, np.ndarray):\n",
    "        neg = neg.detach().cpu().numpy()\n",
    "\n",
    "    if len(pos.shape) < 2:\n",
    "        pos = np.expand_dims(pos, 1)\n",
    "        neg = np.expand_dims(neg, 1)\n",
    "\n",
    "    clf = mixture.GaussianMixture(n_components=n_components, covariance_type=\"full\")\n",
    "    clf.fit(np.concatenate((pos, neg)))\n",
    "\n",
    "    return clf, clf.means_, clf.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_gmm(clf: mixture.GaussianMixture, preds, labels_true):\n",
    "    # if len(preds.shape) > 1 :\n",
    "    #     preds = np.squeeze(preds, 1)\n",
    "\n",
    "    # print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, preds))\n",
    "    # print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, preds))\n",
    "    # print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, preds))\n",
    "    \n",
    "    sample_scores = clf.score_samples(preds)\n",
    "    print(sample_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "def visualize_scalar_dist(s, nbins=30, c='b', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    p, x = np.histogram(s, bins=nbins) # bin it into n = N//10 bins\n",
    "    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers\n",
    "    f = UnivariateSpline(x, p, s=nbins)\n",
    "    ax.plot(x, f(x), c=c)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def visualize_multivar_dist():\n",
    "    pass\n",
    "\n",
    "def visualize_dist(s, *args, **kwargs):\n",
    "    if len(s.shape) > 1 or (len(s.shape) == 2 and s.shape[1] != 1):\n",
    "        visualize_scalar_dist(s, *args, **kwargs)\n",
    "    else:\n",
    "        visualize_multivar_dist(s, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, w, h, d):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes \n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.depth = d\n",
    "        self.classifier = torch.nn.Conv3d(in_channels, num_classes, (1,1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.depth, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,4,1,2, 3)\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, epochs, optimizer, criterion):\n",
    "  # put model in training mode\n",
    "  model.train()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        # labels = batch[\"labels\"].to(device)\n",
    "        dp, gt = batch\n",
    "\n",
    "        # forward pass\n",
    "        out_logits = model(dp)\n",
    "        if out_logits.size() != gt.size():\n",
    "            while out_logits.size() != gt.size():\n",
    "                out_logits = F.interpolate(out_logits, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "            \n",
    "        dice_loss = criterion(out_logits, gt)\n",
    "        dice_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    total_loss = 0 \n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "\n",
    "        dp, gt = batch \n",
    "\n",
    "        out_logits = model(dp)\n",
    "        if out_logits.size() != gt.size():\n",
    "            while out_logits.size() != gt.size():\n",
    "                out_logits = F.interpolate(out_logits, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "            \n",
    "        dice_loss = criterion(out_logits, gt)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import AdamW\n",
    "# from Upstream.nnunet.training.loss_functions.dice_loss import SoftDiceLoss\n",
    "\n",
    "# model = LinearClassifier()\n",
    "\n",
    "# train_dataloader = None\n",
    "\n",
    "\n",
    "# # training hyperparameters\n",
    "# # NOTE: I've just put some random ones here, not optimized at all\n",
    "# # feel free to experiment, see also DINOv2 paper\n",
    "# learning_rate = 5e-5\n",
    "# epochs = 10\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# criterion = SoftDiceLoss()\n",
    "\n",
    "# # put model on GPU (set runtime to GPU in Google Colab)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# train(model, train_dataloader, epochs=epochs, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Probing\n",
    "# train_features == dynamic prompts\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def cast_and_format(*args):\n",
    "    fmt = []\n",
    "    for arg in args:\n",
    "        if not isinstance(arg, np.ndarray):\n",
    "            arg = arg.detach().cpu().numpy()\n",
    "\n",
    "        bsize = arg.shape[0]\n",
    "\n",
    "        if len(arg.shape) > 2:\n",
    "            arg = np.reshape(arg, (bsize, -1))\n",
    "\n",
    "        fmt.append(arg)\n",
    "\n",
    "    return tuple(fmt)\n",
    "\n",
    "def linear_probing(train_features, train_labels):\n",
    "    \n",
    "    train_features, train_labels = cast_and_format(train_features, train_labels)\n",
    "\n",
    "    # classifier = LogisticRegression(random_state=0, C=0.3, max_iter=1000, verbose=1)\n",
    "    classifier = LinearRegression()\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def test_linear_classifier(classifier, features, labels):\n",
    "\n",
    "    features, labels = cast_and_format(features, labels)\n",
    "\n",
    "    predictions = classifier.predict(features)\n",
    "\n",
    "    accuracy = np.mean((labels == predictions).astype(float)) * 100\n",
    "\n",
    "    return predictions, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nnunet.training.network_training.UniSeg_Trainer import UniSeg_Trainer\n",
    "from nnunet.paths import network_training_output_dir\n",
    "from nnunet.run.default_configuration import get_default_configuration\n",
    "\n",
    "def collect_model_info_and_evaluate(model_checkpoint, exp_name = \"UniSeg_Trainer\", network = \"3d_fullres\", task = \"Task097_11task\", network_trainer = \"UniSeg_Trainer\", plans_identifier = \"DoDNetPlans\", fold=0):\n",
    "    # Get the main plans file\n",
    "    plans_file, output_folder_name, dataset_directory, batch_dice, stage, \\\n",
    "        trainer_class = get_default_configuration(exp_name, network, task, network_trainer, plans_identifier)\n",
    "    trainer = trainer_class(plans_file, fold, output_folder=output_folder_name, dataset_directory=dataset_directory,\n",
    "                            batch_dice=batch_dice, stage=stage, unpack_data=True, deterministic=True, fp16=True)\n",
    "    \n",
    "    # Extract the model name from the checkpoint file name\n",
    "    path_checkpoint = os.path.join(output_folder_name, f\"fold_{fold}\",model_checkpoint)        \n",
    "    trainer.load_checkpoint(path_checkpoint)\n",
    "    \n",
    "    # Get the data loaders\n",
    "    tr_gen = trainer.tr_gen\n",
    "    val_gen = trainer.val_gen\n",
    "    len_data = len(tr_gen.generator._data)\n",
    "    outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets = [], [], [], [], [], []\n",
    "    for i in range(len_data):\n",
    "        output, inter_mediate_prompt, dynamic_prompt, task_prompt, features_x, target = trainer.run_iteration(tr_gen, False,False, True)\n",
    "        # output, inter_mediate_prompt, dynamic_prompt, task_prompt, features_x, target = trainer.run_iteration(tr_gen, False,False)\n",
    "        # metric = get_metric(output, target, inter_mediate_prompt, dynamic_prompt, task_prompt, features_x)\n",
    "        # Possibly move to CPU\n",
    "        outputs.append(output)\n",
    "        inter_mediate_prompts.append(inter_mediate_prompt)\n",
    "        dynamic_prompts.append(dynamic_prompt)\n",
    "        task_prompts.append(task_prompt)\n",
    "        features_xs.append(features_x)\n",
    "        # target is a list\n",
    "        targets.append(target)\n",
    "\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "    return tr_gen, trainer, outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "I am running the following nnUNet: 3d_fullres\n",
      "My trainer class is:  <class 'nnunet.training.network_training.UniSeg_Trainer.UniSeg_Trainer'>\n",
      "For that I will be using the following configuration:\n",
      "num_classes:  2\n",
      "modalities:  {0: ['CT']}\n",
      "use_mask_for_norm OrderedDict([(0, False), (1, False), (2, False), (3, False)])\n",
      "keep_only_largest_region None\n",
      "min_region_size_per_class None\n",
      "min_size_per_class None\n",
      "normalization_schemes None\n",
      "stages...\n",
      "\n",
      "stage:  0\n",
      "{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 192, 192]), 'median_patient_size_in_voxels': array([ 29, 113, 133]), 'current_spacing': array([3. , 1.5, 1.5]), 'original_spacing': array([3. , 1.5, 1.5]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}\n",
      "\n",
      "I am using stage 0 from these plans\n",
      "I am using sample dice + CE loss\n",
      "\n",
      "I am using data from this folder:  /data/nnUNet_preprocessed/Task097_11task/DoDNetData_plans\n",
      "###############################################\n",
      "task_class {0: 3, 1: 3, 2: 3, 3: 3, 4: 2, 5: 2, 6: 2, 7: 2, 8: 2, 9: 4, 10: 2}\n",
      "num batches per epoch: 550\n",
      "total task num 11\n",
      "/\n",
      "copy code successfully!\n",
      "2024-05-06 15:30:16.126562: loading checkpoint /data/nnUNet_trained_models/UniSeg_Trainer/3d_fullres/Task091_MOTS/UniSeg_Trainer__DoDNetPlans/fold_0/model_latest.model train= True\n",
      "process_plans [ 64 192 192]\n",
      "loading dataset\n",
      "2024-05-06 15:30:16.136176: Using splits from existing split file: /data/nnUNet_preprocessed/Task097_11task/splits_final.pkl\n",
      "2024-05-06 15:30:16.161284: The split file contains 1 splits.\n",
      "2024-05-06 15:30:16.161386: Desired fold for training: 0\n",
      "2024-05-06 15:30:16.161453: This split has 920 training and 235 validation cases.\n",
      "task_data_list, [104, 168, 242, 224, 100, 50, 32, 0, 0, 0, 0]\n",
      "after removal: [104, 168, 242, 224, 100, 50, 32, 0, 0, 0]\n",
      "after removal: [104, 168, 242, 224, 100, 50, 32, 0, 0]\n",
      "after removal: [104, 168, 242, 224, 100, 50, 32, 0]\n",
      "after removal: [104, 168, 242, 224, 100, 50, 32]\n",
      "task_data_list, [104, 168, 242, 224, 100, 50, 32]\n",
      "Using UniSeg 3D Dataloader! with lock\n",
      "task_data_list, [27, 42, 61, 57, 26, 13, 9, 0, 0, 0, 0]\n",
      "after removal: [27, 42, 61, 57, 26, 13, 9, 0, 0, 0]\n",
      "after removal: [27, 42, 61, 57, 26, 13, 9, 0, 0]\n",
      "after removal: [27, 42, 61, 57, 26, 13, 9, 0]\n",
      "after removal: [27, 42, 61, 57, 26, 13, 9]\n",
      "task_data_list, [27, 42, 61, 57, 26, 13, 9]\n",
      "Using UniSeg 3D Dataloader! with lock\n",
      "unpacking dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/nnUNet_trained_models/UniSeg_Trainer/3d_fullres/Task091_MOTS/UniSeg_Trainer__DoDNetPlans/fold_0/model_latest.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tr_gen, trainer, outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_model_info_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUniSeg_Trainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m3d_fullres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTask097_11task\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork_trainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUniSeg_Trainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplans_identifier\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDoDNetPlans\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mcollect_model_info_and_evaluate\u001b[0;34m(model_checkpoint, exp_name, network, task, network_trainer, plans_identifier, fold)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract the model name from the checkpoint file name\u001b[39;00m\n\u001b[1;32m     13\u001b[0m path_checkpoint \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,model_checkpoint)        \n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get the data loaders\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tr_gen \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtr_gen\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nnunet/training/network_training/network_trainer.py:318\u001b[0m, in \u001b[0;36mNetworkTrainer.load_checkpoint\u001b[0;34m(self, fname, train)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_to_log_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain=\u001b[39m\u001b[38;5;124m\"\u001b[39m, train)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwas_initialized:\n\u001b[0;32m--> 318\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# saved_model = torch.load(fname, map_location=torch.device('cuda', torch.cuda.current_device()))\u001b[39;00m\n\u001b[1;32m    320\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fname, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nnunet/training/network_training/UniSeg_Trainer.py:131\u001b[0m, in \u001b[0;36mUniSeg_Trainer.initialize\u001b[0;34m(self, training, force_load_plans)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack_data:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munpacking dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m     \u001b[43munpack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfolder_with_preprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nnunet/training/dataloading/dataset_loading.py:72\u001b[0m, in \u001b[0;36munpack_dataset\u001b[0;34m(folder, threads, key)\u001b[0m\n\u001b[1;32m     70\u001b[0m p\u001b[38;5;241m.\u001b[39mmap(convert_to_npy, \u001b[38;5;28mzip\u001b[39m(npz_files, [key] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(npz_files)))\n\u001b[1;32m     71\u001b[0m p\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 72\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py:666\u001b[0m, in \u001b[0;36mPool.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_handler\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool:\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:47\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"/data/nnUNet_trained_models/UniSeg_Trainer/3d_fullres/Task091_MOTS/UniSeg_Trainer__DoDNetPlans/fold_0/model_latest.model\"\n",
    "tr_gen, trainer, outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets = collect_model_info_and_evaluate(model_checkpoint, exp_name = \"UniSeg_Trainer\", network = \"3d_fullres\", task = \"Task097_11task\", network_trainer = \"UniSeg_Trainer\", plans_identifier = \"DoDNetPlans\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(s, nbins=30, c='b'):\n",
    "    p, x = np.histogram(s, bins=nbins) # bin it into n = N//10 bins\n",
    "    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers\n",
    "    f = UnivariateSpline(x, p, s=nbins)\n",
    "    plt.plot(x, f(x), c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lst = [t[-2] for t in targets]\n",
    "\n",
    "tps = torch.vstack(task_prompts)\n",
    "gts = torch.vstack(target_lst)\n",
    "classifier = linear_probing(tps, gts)\n",
    "predictions, accuracy = test_linear_classifier(classifier, tps, gts)\n",
    "\n",
    "print(f\"Linear Probing Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set, neg_set = extract(tps, gts)\n",
    "pos_set = pos_set.detach().cpu().numpy().reshape(-1, 1)\n",
    "neg_set = neg_set.detach().cpu().numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dist(np.concatenate((pos_set, neg_set)))\n",
    "plt.close()\n",
    "plot_dist(pos_set, c='b')\n",
    "plot_dist(neg_set, c='r')\n",
    "plt.show()\n",
    "plt.savefig(\"/workspace/dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "gmm, means, covariances = fit_gmm(pos_set, neg_set)\n",
    "\n",
    "print(means)\n",
    "print(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer the assignment gmm made by visualization of distribution\n",
    "pos_gt = np.zeros_like(pos_set)#.squeeze(1)\n",
    "neg_gt = np.ones_like(neg_set)#.squeeze(1)\n",
    "\n",
    "pos_gmm_acc = evaluate_gmm(gmm, pos_set, pos_gt)\n",
    "neg_gmm_acc = evaluate_gmm(gmm, neg_set, neg_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
