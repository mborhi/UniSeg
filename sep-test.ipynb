{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def extract(dp, gt, keep_dims=True):\n",
    "\n",
    "    while dp.size() != gt.size():\n",
    "        dp = F.interpolate(dp, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "\n",
    "    print(dp.size())\n",
    "    vpos_sets, vneg_sets = [],[]\n",
    "    for i in range(dp.size(0)):\n",
    "        pos_voxels = torch.argwhere(gt[i, 0] > 0)\n",
    "        neg_voxels = torch.argwhere(gt[i, 0] == 0)\n",
    "        \n",
    "        pos_set = dp[i, :, pos_voxels[:, 0], pos_voxels[:, 1], pos_voxels[:, 2]]\n",
    "        neg_set = dp[i, :, neg_voxels[:, 0], neg_voxels[:, 1], neg_voxels[:, 2]]\n",
    "\n",
    "        vpos_sets.append(pos_set)\n",
    "        vneg_sets.append(neg_set)\n",
    "\n",
    "    print([v.size() for v in vpos_sets])\n",
    "    vpos_sets = torch.concat(vpos_sets, -1)    \n",
    "    vneg_sets = torch.concat(vneg_sets, -1)\n",
    "\n",
    "    if keep_dims: \n",
    "        return vpos_sets, vneg_sets\n",
    "    \n",
    "    return vpos_sets.mean(dim=0), vneg_sets.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gmm(pos, neg, n_components=2):\n",
    "    if isinstance(n_components, str):\n",
    "        # infer the number of components\n",
    "        pass \n",
    "\n",
    "    if not isinstance(pos, np.ndarray):\n",
    "        pos = pos.detach().cpu().numpy()\n",
    "\n",
    "    if not isinstance(neg, np.ndarray):\n",
    "        neg = neg.detach().cpu().numpy()\n",
    "\n",
    "    if len(pos.shape) < 2:\n",
    "        pos = np.expand_dims(pos, 1)\n",
    "        neg = np.expand_dims(neg, 1)\n",
    "\n",
    "    clf = mixture.GaussianMixture(n_components=n_components, covariance_type=\"full\")\n",
    "    clf.fit(np.concatenate((pos, neg)))\n",
    "\n",
    "    return clf, clf.means_, clf.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_gmm(clf: mixture.GaussianMixture, preds, labels_true):\n",
    "    if len(preds.shape) != 1 :\n",
    "        preds = np.squeeze(preds, 1)\n",
    "\n",
    "    if len(labels_true.shape) != 1:\n",
    "        labels_true = np.squeeze(labels_true, 1)\n",
    "\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, preds))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, preds))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, preds))\n",
    "    \n",
    "    # sample_scores = clf.score_samples(preds)\n",
    "    # print(sample_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "def visualize_scalar_dist(s, nbins=30, c='b', ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    p, x = np.histogram(s, bins=nbins) # bin it into n = N//10 bins\n",
    "    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers\n",
    "    f = UnivariateSpline(x, p, s=nbins)\n",
    "    ax.plot(x, f(x), c=c)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def visualize_multivar_dist():\n",
    "    pass\n",
    "\n",
    "def visualize_dist(s, *args, **kwargs):\n",
    "    if len(s.shape) > 1 or (len(s.shape) == 2 and s.shape[1] != 1):\n",
    "        visualize_scalar_dist(s, *args, **kwargs)\n",
    "    else:\n",
    "        visualize_multivar_dist(s, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, w, h, d):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes \n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.depth = d\n",
    "        self.classifier = torch.nn.Conv3d(in_channels, num_classes, (1,1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.depth, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,4,1,2, 3)\n",
    "        logits = self.classifier(embeddings)\n",
    "        \n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, epochs, optimizer, criterion):\n",
    "  # put model in training mode\n",
    "  model.train()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        # pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        # labels = batch[\"labels\"].to(device)\n",
    "        dp, gt = batch\n",
    "\n",
    "        # forward pass\n",
    "        out_logits = model(dp)\n",
    "        if out_logits.size() != gt.size():\n",
    "            while out_logits.size() != gt.size():\n",
    "                out_logits = F.interpolate(out_logits, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "            \n",
    "        dice_loss = criterion(out_logits, gt)\n",
    "        dice_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    total_loss = 0 \n",
    "    for batch_idx, batch in enumerate(test_dataloader):\n",
    "\n",
    "        dp, gt = batch \n",
    "\n",
    "        out_logits = model(dp)\n",
    "        if out_logits.size() != gt.size():\n",
    "            while out_logits.size() != gt.size():\n",
    "                out_logits = F.interpolate(out_logits, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "            \n",
    "        dice_loss = criterion(out_logits, gt)\n",
    "\n",
    "\n",
    "def run_linear_clf():\n",
    "    pass\n",
    "    # from torch.optim import AdamW\n",
    "    # from Upstream.nnunet.training.loss_functions.dice_loss import SoftDiceLoss\n",
    "\n",
    "    # model = LinearClassifier()\n",
    "\n",
    "    # train_dataloader = None\n",
    "\n",
    "\n",
    "    # # training hyperparameters\n",
    "    # # NOTE: I've just put some random ones here, not optimized at all\n",
    "    # # feel free to experiment, see also DINOv2 paper\n",
    "    # learning_rate = 5e-5\n",
    "    # epochs = 10\n",
    "\n",
    "    # optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    # criterion = SoftDiceLoss()\n",
    "\n",
    "    # # put model on GPU (set runtime to GPU in Google Colab)\n",
    "    # device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model.to(device)\n",
    "\n",
    "    # train(model, train_dataloader, epochs=epochs, optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Probing\n",
    "# train_features == dynamic prompts\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, matthews_corrcoef\n",
    "\n",
    "def cast_and_format(*args):\n",
    "    fmt = []\n",
    "    for arg in args:\n",
    "        if not isinstance(arg, np.ndarray):\n",
    "            arg = arg.detach().cpu().numpy()\n",
    "\n",
    "        bsize = arg.shape[0]\n",
    "\n",
    "        # if len(arg.shape) > 2:\n",
    "        #     arg = np.reshape(arg, (bsize, -1))\n",
    "\n",
    "        if len(arg.shape) > 2:\n",
    "            arg = arg.reshape([-1, 1])\n",
    "\n",
    "        fmt.append(arg)\n",
    "\n",
    "    return tuple(fmt)\n",
    "\n",
    "def linear_probing(train_features, train_labels):\n",
    "    while train_features.size() != train_labels.size():\n",
    "        train_features = F.interpolate(train_features, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "\n",
    "    train_features, train_labels = cast_and_format(train_features, train_labels)\n",
    "\n",
    "    classifier = LogisticRegression(random_state=0, C=0.3, max_iter=1000, verbose=1)\n",
    "    # classifier = LinearRegression()\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def test_linear_classifier(classifier, features, labels):\n",
    "    # match size first\n",
    "\n",
    "    while features.size() != labels.size():\n",
    "        features = F.interpolate(features, scale_factor=(2, 2, 2), mode='trilinear')\n",
    "    \n",
    "    features, labels = cast_and_format(features, labels)\n",
    "\n",
    "    predictions = classifier.predict(features)\n",
    "\n",
    "    accuracy = np.mean((labels == predictions).astype(float)) * 100\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    auc_score = roc_auc_score(labels, predictions)\n",
    "\n",
    "    return predictions, {\"Accuracy\": accuracy, \"F1\": f1, \"AUC\": auc_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunet.training.network_training.UniSeg_Trainer import UniSeg_Trainer\n",
    "from nnunet.paths import network_training_output_dir\n",
    "from nnunet.run.default_configuration import get_default_configuration\n",
    "\n",
    "def collect_model_info_and_evaluate(model_checkpoint, exp_name = \"UniSeg_Trainer\", network = \"3d_fullres\", task = \"Task097_11task\", network_trainer = \"UniSeg_Trainer\", plans_identifier = \"DoDNetPlans\", fold=0):\n",
    "    # Get the main plans file\n",
    "    plans_file, output_folder_name, dataset_directory, batch_dice, stage, \\\n",
    "        trainer_class = get_default_configuration(exp_name, network, task, network_trainer, plans_identifier)\n",
    "    trainer = trainer_class(plans_file, fold, output_folder=output_folder_name, dataset_directory=dataset_directory,\n",
    "                            batch_dice=batch_dice, stage=stage, unpack_data=True, deterministic=True, fp16=True)\n",
    "    \n",
    "    # Extract the model name from the checkpoint file name\n",
    "    path_checkpoint = os.path.join(output_folder_name, f\"fold_{fold}\",model_checkpoint)        \n",
    "    trainer.load_checkpoint(path_checkpoint)\n",
    "    \n",
    "    # Get the data loaders\n",
    "    tr_gen = trainer.tr_gen\n",
    "    val_gen = trainer.val_gen\n",
    "    len_data = len(tr_gen.generator._data)\n",
    "    outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets = [], [], [], [], [], []\n",
    "    for i in range(len_data):\n",
    "        output, inter_mediate_prompt, dynamic_prompt, task_prompt, features_x, target = trainer.run_iteration(tr_gen, False,False, True)\n",
    "        # output, inter_mediate_prompt, dynamic_prompt, task_prompt, features_x, target = trainer.run_iteration(tr_gen, False,False)\n",
    "        # metric = get_metric(output, target, inter_mediate_prompt, dynamic_prompt, task_prompt, features_x)\n",
    "        # Possibly move to CPU\n",
    "        outputs.append(output)\n",
    "        inter_mediate_prompts.append(inter_mediate_prompt)\n",
    "        dynamic_prompts.append(dynamic_prompt)\n",
    "        task_prompts.append(task_prompt)\n",
    "        features_xs.append(features_x)\n",
    "        # target is a list\n",
    "        targets.append(target)\n",
    "\n",
    "        if i == 2:\n",
    "            break\n",
    "\n",
    "    return tr_gen, trainer, outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"/data/nnUNet_trained_models/UniSeg_Trainer/3d_fullres/Task091_MOTS/UniSeg_Trainer__DoDNetPlans/fold_0/model_latest.model\"\n",
    "tr_gen, trainer, outputs, inter_mediate_prompts, dynamic_prompts, task_prompts, features_xs,targets = collect_model_info_and_evaluate(model_checkpoint, exp_name = \"UniSeg_Trainer\", network = \"3d_fullres\", task = \"Task097_11task\", network_trainer = \"UniSeg_Trainer\", plans_identifier = \"DoDNetPlans\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(s, nbins=30, c='b'):\n",
    "    p, x = np.histogram(s, bins=nbins) # bin it into n = N//10 bins\n",
    "    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers\n",
    "    f = UnivariateSpline(x, p, s=nbins)\n",
    "    plt.plot(x, f(x), c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lst = [t[-2] for t in targets]\n",
    "gts = torch.vstack(target_lst)\n",
    "cast_and_format(gts)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lst = [t[-2] for t in targets]\n",
    "\n",
    "tps = torch.vstack(task_prompts)\n",
    "gts = torch.vstack(target_lst)\n",
    "print(gts.size())\n",
    "classifier = linear_probing(tps, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, metric_res = test_linear_classifier(classifier, tps, gts)\n",
    "\n",
    "for metric_name, res in metric_res.items():\n",
    "    print(f\"Linear Probing {metric_name}: {res}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set, neg_set = extract(tps, gts)\n",
    "pos_set = pos_set.detach().cpu().numpy().reshape(-1, 1)\n",
    "neg_set = neg_set.detach().cpu().numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_set.shape\n",
    "# neg_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dist(np.concatenate((pos_set, neg_set)))\n",
    "plt.close()\n",
    "plot_dist(pos_set, c='b')\n",
    "plot_dist(neg_set, c='r')\n",
    "plot_dist(np.concatenate((pos_set, neg_set)), c='purple')\n",
    "plt.show()\n",
    "plt.savefig(\"/workspace/dist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "gmm, means, covariances = fit_gmm(pos_set, neg_set)\n",
    "\n",
    "print(means)\n",
    "print(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer the assignment gmm made by visualization of distribution\n",
    "pos_gt = np.zeros_like(pos_set)#.squeeze(1)\n",
    "neg_gt = np.ones_like(neg_set)#.squeeze(1)\n",
    "\n",
    "pos_gmm_acc = evaluate_gmm(gmm, pos_set, pos_gt)\n",
    "neg_gmm_acc = evaluate_gmm(gmm, neg_set, neg_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(*cast_and_format(tps))\n",
    "\n",
    "print(f\"cluster centers: {kmeans.cluster_centers_}\")\n",
    "pos_pred = kmeans.predict(pos_set)\n",
    "print(f\"{np.mean(1-pos_pred)}\")\n",
    "\n",
    "neg_pred = kmeans.predict(neg_set)\n",
    "print(f\"{np.mean(neg_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "X = np.concatenate((pos_set, neg_set), 0)\n",
    "y = np.concatenate((np.ones_like(pos_set), np.zeros_like(neg_set)), 0)\n",
    "neigh.fit(X, y)\n",
    "print(neigh.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
